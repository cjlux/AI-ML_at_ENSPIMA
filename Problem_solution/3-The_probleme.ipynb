{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:10pt\">AI @ ENSPIMA_2022-2023_v1.0_Jean-Luc CHARLES (Jean-Luc.charles@ensam.eu)_CC BY-SA 4.0</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Problem-based learning\n",
    "# Training a neural network to diagnose bearing faults - part 1 / 3\n",
    "\n",
    "### Targeted learning objectives:\n",
    "Part 1:<br>\n",
    "- Know how to load files in *Matlab MAT-file* format with *Python*.\n",
    "- Know how to dimension and fill numpy ndarrays with the data of the `.mat` files\n",
    "- Know how to display a grid of data plots\n",
    "- Know how to store the numpy ndarrays in a `.npz` file\n",
    "\n",
    "Part 2:<br>\n",
    "- Know how to load a `.npz` into numpy ndarrays\n",
    "- Know how to process the temporal dataset to get a spectral dataset.\n",
    "- Know how to display a grid of spectra plots.\n",
    "\n",
    "Part 3:<br>\n",
    "- Know how to train/operate a DNN to diagnose bearing faults using a labeled temporal dataset.\n",
    "- The problem part of the APP: Know how to train/operate a DNN to diagnose bearing faults using a labeled temporal dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<span style=\"color:brown;font-family:arial;font-size:12pt\"> \n",
    "It is important to use a <span style=\"font-weight:bold;\">Python Virtual Environment</span> (PVE) for your main Python projects: <br>\n",
    "    a PVE makes it possible to control for each project the versions of the Python interpreter and the \"sensitive\" modules (like tensorflow).<br><br>\n",
    "    All the notebooks your work on must be loaded into a jupyter-notebook or a jupyter-lab launched in the PVE \n",
    "    <b><span style=\"color: rgb(100, 151, 202);\" >pyml-pm</span></b> specially created for the session.<br>\n",
    "</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "# Delete the (numerous) warning messages from the **tensorflow** module:\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"Python    : {sys.version.split()[0]}\")\n",
    "print(f\"tensorflow: {tf.__version__} incluant keras {keras.__version__}\")\n",
    "print(f\"numpy     : {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed of the random generators used by tensorflow:\n",
    "SEED = 1234"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder: The bearing data set was obtained under the experimental conditions\n",
    "- under normal condition (N)\n",
    "- with outer race fault (OF)\n",
    "- with inner race fault (IF)\n",
    "- with roller fault (OF).\n",
    "\n",
    "|class label|Fault type|Fault diameter|\n",
    "|:---------:|:--------:|-------------:|\n",
    "| 1         | N        | 0            |\n",
    "| 2         | RF       | 0.18         |\n",
    "| 3         | RF       | 0.36         |\n",
    "| 4         | RF       | 0.54         |\n",
    "| 5         | IF       | 0.36         |\n",
    "| 6         | IF       | 0.36         |\n",
    "| 7         | IF       | 0.54         |\n",
    "| 8         | OF       | 0.18         |\n",
    "| 9         | OF       | 0.36         |\n",
    "| 10        | OF       | 0.54         |\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 $-$ A first try to train the neural network with the temporal dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 $-$ Load the *CWRU* data and define some useful objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npzfile = np.load('CWRU_dadaset.npz')\n",
    "A, B, C = npzfile.values()\n",
    "full_dataset = (A, B, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimensions of A are (#health_conditions, #samples, #data_points).\n",
    "\n",
    "Let's define:\n",
    "- `H` $\\leadsto$ the total number of health conditions\n",
    "- `S` $\\leadsto$ the total number of samples per health contion\n",
    "- `N` $\\leadsto$ the total number of data points per sample\n",
    "- `L` $\\leadsto$ the total number of load cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H, S, N = A.shape\n",
    "L = len(full_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the list of the health conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the list of the health condition labels:\n",
    "health_cond = ['N']\n",
    "for def_type in 'RF', 'IF', 'OF':\n",
    "    for size in '18', '36', '54':\n",
    "        health_cond.append(f\"{def_type}.{size}\")\n",
    "print(f\"list of {len(health_cond)} health conditions:\", health_cond)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 $-$ Prepare the labeled data for the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two actions to do:\n",
    "- merge the 200 samples for each of the 10 health conditions and each of the 3 load cases into a single array of 200 $\\times$ 10 $\\times$ 3 = 6000 samples,\n",
    "- build the array of the cooreponding 6000 labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define the arrays with the right shapes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_full = np.ndarray((L*H*S, N), dtype='float')  # the array of the samples\n",
    "y_full = np.ndarray((L*H*S,), dtype='uint8')    # the array of the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify the shape of the arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_full.shape, y_full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we fill `x_full` with samples and `y_full` with corresponding labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for data in (A, B, C):       # browse the 3 load cases\n",
    "    for h in range(H):       # browse the 10 health conditions -> the labels\n",
    "        for s in range(S):   # browse the 200 samples\n",
    "            x_full[i] = data[h, s]\n",
    "            y_full[i] = h    # the label is given by the health condition loop variable\n",
    "            i += 1                                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify, let's plot the sample of rank 10 in `x_full`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(x_full[10]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization of the temporal samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalize the samples of `x_full` by dividing each one by the max of its absolute values.<br>\n",
    "There are two ways to do this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a/ Version with an **explicit loop** to browse through all the samples of `x_full`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x_full)):\n",
    "    x_full[i] = x_full[i]/np.abs(x_full[i]).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look now at the sample of rank 10 in `x_full`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(x_full[10]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b/ the numpy **vectorized** style (more efficient):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_full = x_full/np.abs(x_full).max(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look now at the sample of rank 10 in `x_full`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(x_full[10]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\leadsto$ the values of `x_full` are now all in the range [-1, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Split the full dataset into train and test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we just use the `train_test_split` function from `sklean.model_slection`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, lab_train, lab_test = train_test_split(x_full, y_full, \n",
    "                                                        stratify=y_full,      # use y_full to evenly distribute all classes \n",
    "                                                                              # in the train and test dadasets\n",
    "                                                        test_size=0.5,        # 50 % test, 50% train \n",
    "                                                        random_state=SEED, \n",
    "                                                        shuffle=True)         # shuffe randomly the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Transform labels to *one-hot* format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "# 'one-hot' encoding' des labels :\n",
    "y_train = to_categorical(lab_train)\n",
    "y_test  = to_categorical(lab_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a recap the the shapes of the arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, x_test.shape, lab_train.shape, lab_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 $-$ Build the Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will build a dense neural network wit this structure:\n",
    "    \n",
    "     Input layer : 1900 inputs\n",
    "     Hidden layer 'H1' : 1900 neurones, activation fucntion: relu                                                    \n",
    "     Hidden layer 'H2' : 600  neurones, activation fucntion: relu                          \n",
    "     Hidden layer 'H3' : 200  neurones, activation fucntion: relu                         \n",
    "     Hidden layer 'H4' : 100  neurones, activation fucntion: relu\n",
    "     Output layer      : 10   neurones, activation fucntion: softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "\n",
    "# set the seed of the random generators used by tensorflow:\n",
    "SEED = 1234\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# the 5 lines to build the neural network:\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(N,), name='Input'))\n",
    "model.add(Dense(N, activation='relu', name='H1'))\n",
    "model.add(Dense(600, activation='relu', name='H2'))\n",
    "model.add(Dense(200, activation='relu', name='H3'))\n",
    "model.add(Dense(100, activation='relu', name='H4'))\n",
    "model.add(Dense(H, activation='softmax', name='Output'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the initial values of the network weights if we want to reload later the network to its initial state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether the folder 'weights' exists and create it if needed:\n",
    "if not os.path.isdir(\"weights\"): os.mkdir(\"weights\")\n",
    "\n",
    "# Save the initial DNN (random) weights:\n",
    "key = 'CWRU_temporal_init'\n",
    "model.save_weights(os.path.join('weights', key))\n",
    "\n",
    "# Display the created files:\n",
    "files=[os.path.join(\"weights\",f) for f in os.listdir(\"weights\") if f.startswith(key)]\n",
    "for f in files: print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the neural network with `x_train` & `y_train` as the labeled dataset and `x_test` & `y_test` as the validation labeled dataset to use at the end of each epoch to mesure the network performance.<br>\n",
    "To avoid any *over-fitting* we use the `EarlyStopping` callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "callbacks_list = [ \n",
    "    EarlyStopping(monitor='loss',   # \n",
    "                  patience=3,           #\n",
    "                  restore_best_weights=True,\n",
    "                  verbose=1)\n",
    "]\n",
    "\n",
    "# in case we execute this cell several times, we can re-initialize \n",
    "# the network to its initial state if we want to compare the workouts...\n",
    "key = 'CWRU_temporal_init'\n",
    "model.load_weights(os.path.join('weights', key)) \n",
    "\n",
    "# set the seed of the random generators inolved by tensorflow:\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# train the DNN:\n",
    "hist = model.fit(x_train, y_train,\n",
    "                 validation_data=(x_test, y_test), \n",
    "                 epochs=50, \n",
    "                 batch_size=64,                     # number of samples in the batch\n",
    "                 callbacks = callbacks_list)\n",
    "\n",
    "from utils.tools import plot_loss_accuracy\n",
    "plot_loss_accuracy(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compute the trained network predictions for the test datatset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict(x_test)          # restults is an array of probabilities vectors\n",
    "inferences = results.argmax(axis=-1)     # extract the highest probablities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can plot the **confusion matrix** to  see if the networkd is well trained or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "axis = plt.axes()\n",
    "ConfusionMatrixDisplay.from_predictions(lab_test, inferences, \n",
    "                                        ax=axis,\n",
    "                                        display_labels=health_cond, \n",
    "                                        xticks_rotation='vertical',\n",
    "                                        colorbar=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all the bearing defaults are recognized with a good score...<br>\n",
    "$\\leadsto$ the next step is to try to train the network with the spectral datasets computed from the temporal samples to see if it's better ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 $-$ Finally the problem: Train the neural network with the Fourier spectrum of the temporal data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 $-$ Compute the spectral datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See **3.1 − Compute the spectral datasets** in the notebook *2-process_CWRU_data.ipynb* to help you to do the work...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H, S, N = A.shape\n",
    "print(f\"array A has <{S}> samples of <{N}> data point for each of the <{H}> health conditions \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spectra are computed with [numpy.fft.rfft](https://numpy.org/doc/stable/reference/generated/numpy.fft.rfft.html)<br>\n",
    "On the web page, you can see how to compute the size of the spectrum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if S % 2 == 0:\n",
    "    N_spectrum = int(N/2+1)\n",
    "else:\n",
    "    N_spectrum = int((N+1)/2)\n",
    "print(f\"size of spectra: {N_spectrum}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you must define and dimension 3 ndarrays of `floats` to store the spectra of the 3 temporal data arrays.<br>\n",
    "For the dimensions, you must use `H`, `S` and `N_spectrum`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_spectrum = \n",
    "B_spectrum = \n",
    "C_spectrum = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can compute the spectra with the `np.fft.rfft` function and fill in the 3 arrays (:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 $-$ Prepare the data set for supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw previously, we can keep only the first 400 spectral points in each sample, so you must define `x_full` and `y_full` with the appropriate dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_spectrum = 400\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_full.shape, y_full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can fill `x_full` and `y_full` appropriately (see **4.2 −\n",
    "Prepare the labeled data for the training** to get some help):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look on the spectrum of rank 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(x_full[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Split the full dataset into train and test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see **4.3 Split the full dataset into train and test datasets** to get some help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Transform labels to *one-hot* format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see **4.4 Transform labels to one-hot format** to get some help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, x_test.shape, lab_train.shape, lab_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 $-$ Build the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "\n",
    "# set the seed of the random generators used by tensorflow:\n",
    "SEED = 1234\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# the 5 lines to build the neural network:\n",
    "modelS = Sequential()\n",
    "......\n",
    "......\n",
    "......\n",
    "modelS.compile(loss='categorical_crossentropy', optimizer='adam',  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelS.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether the folder 'weights' exists and create it if needed:\n",
    "if not os.path.isdir(\"weights\"): os.mkdir(\"weights\")\n",
    "\n",
    "# Save the initial DNN (random) weights:\n",
    "key = 'CWRU_spectral_init'\n",
    "modelS.save_weights(os.path.join('weights', key))\n",
    "\n",
    "# Display the created files:\n",
    "files=[os.path.join(\"weights\",f) for f in os.listdir(\"weights\") if f.startswith(key)]\n",
    "for f in files: print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the `modelS` network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether the folder 'models' exists and create it if needed:\n",
    "if not os.path.exists(\"models\"): os.mkdir(\"models\")\n",
    "\n",
    "# save the trained DNN structure + wieghts:\n",
    "key = 'CWRU_spectral_init'\n",
    "modelS.save(os.path.join('models', key) )\n",
    "\n",
    "# Display the created files:\n",
    "files=[os.path.join(\"models\",f) for f in os.listdir(\"models\") if f.startswith(key)]\n",
    "for f in files: print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compute the trained network predictions for the test datatset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can plot the **confusion matrix** to  see if the networkd is well trained or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
