{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:10pt\">AI @ ENSPIMA_2022-2023_v1.0_Jean-Luc <span style=\"font-size:10pt\">AI @ ENSPIMA_2022-2023_v1.0_Jean-Luc CHARLES (Jean-Luc.charles@ensam.eu)_CC BY-SA 4.0</span> (Jean-Luc.charles@ensam.eu)_CC BY-SA 4.0</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning with Python tensorflow2/keras modules:\n",
    "\n",
    "# Train/operate a dense neural network for the recognition of handwritten digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<span style=\"color:brown;font-family:arial;font-size:12pt\"> \n",
    "It is important to use a <span style=\"font-weight:bold;\">Python Virtual Environment</span> (PVE) for your main Python projects: <br>\n",
    "    a PVE makes it possible to control for each project the versions of the Python interpreter and the \"sensitive\" modules (like tensorflow).</span></div>\n",
    "\n",
    "All the notebooks in this directory must be loaded into a `jupyter-notebook` or a `jupyter-lab` launched in the PVE <b><span style=\"color: rgb(200, 151, 102);\" >pyml-pm</span></b> specially created for the session.<br>\n",
    "They should be worked in this order:\n",
    "- `ML1_MNIST_en.ipynb`: check that the <b><span style=\"color: rgb(200, 151, 102);\">pyml-pm</span></b> EVP is fuly operationnal, load and use the data from the MNIST database (images and labels).\n",
    "- `ML2_DNN_part1_en.ipynb`: build a Dense Neural Network (DNN), train it with data from the MNIST and evaluate its performance.\n",
    "- `ML2_DNN_part2_en.ipynb`: reload a previously trained DNN and evaluate its performance with the MNIST test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Targeted learning objectives:\n",
    "- Know how to build a dense neural network with the Python modules **tensorflow** and **keras**.\n",
    "- Know how to train a dense network with data from the MNIST bank.\n",
    "- Know how to display the training performance curves.\n",
    "- Know how to save the structure and the weights of the trained network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Verify importing Python modules\n",
    "The **keras** module which allows high-level manipulation of **tensorflow** objects is integrated in the **tensorflow** (tf) module since version 2. <br>\n",
    "The **tf.keras** module documentation to consult is here: https://www.tensorflow.org/api_docs/python/tf/keras.\n",
    "\n",
    "Importing the `tensorflow` module in the cell below may generate some warning messages...<br>\n",
    "if errors appear they must be corrected, possibly by recreating your PVE <b><span style=\"color: rgb(200, 51, 102);\">pyml-pm</span></b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, cv2\n",
    "\n",
    "# Delete the (numerous) warning messages from the **tensorflow** module:\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"Python    : {sys.version.split()[0]}\")\n",
    "print(f\"tensorflow: {tf.__version__} incluant keras {keras.__version__}\")\n",
    "print(f\"numpy     : {np.__version__}\")\n",
    "print(f\"OpenCV    : {cv2.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding matplotlib plots in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense network structure to build\n",
    "In this step you will build a **dense network**, with:\n",
    "- an **input layer** of 784 values in the range [0 ; 1.]<br>\n",
    "(the pixels of the MNIST 28 $\\times$ 28 images put in the form of a vector of 784 normalized `float` numbers),\n",
    "- a **hidden layer** of 784 neurons using the `relu` activation function,\n",
    "- an **output layer** with 10 neurons, for the classification of images into 10 classes associated with the digits {0,1,2...9}, using the `softmax` activation function adapted to classification problems .\n",
    "\n",
    "<p style=\"text-align:center; font-style:italic; font-size:12px;\">\n",
    "    <img src=\"img/archiNetwork.png\" alt=\"archiNetwork.png\" style=\"width:900px;\"><br>\n",
    "    [image credit: JLC]\n",
    "</p>\n",
    "\n",
    "Remarks :\n",
    "- Each neuron of the first hidden layer receives 785 inputs: the 784 values $x_i$ of the pixels of the image plus the bias.\n",
    "- $\\leadsto$ There are therefore 785 unknowns for each neuron: the 784 weights $w_i$ assigned to each input $x_i$, plus the weight $b$ assigned to input $-1$.\n",
    "- $\\leadsto$ there are therefore 785 $\\times$ 784 unknowns for the hidden layer and 785 $\\times$ 10 unknowns for the output layer: i.e. a total of 623290 unknowns whose value must be optimized by the algorithm d network learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work to do\n",
    "### 1 - Load MNIST images and define important parameters\n",
    "### 2 - Pre-process MNIST images and labels\n",
    "### 3 - Build the dense neural network\n",
    "### 4 - A first network training test\n",
    "### 5 - Train the network and measure its performance at each *epoch*\n",
    "### 6 - Train the network, measure its performance and manage the *over-fit*\n",
    "### 7 - Save the trained network.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Load MNIST images and define important parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The work of loading MNIST images has been processed in the *notebook* `ML1_MNIST.ipynb`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(im_train, lab_train), (im_test, lab_test) = tf.keras.datasets.mnist.load_data()\n",
    "print(\"im_train -> shape:\", im_train.shape, \", dtype:\", im_train.dtype,)\n",
    "print(\"im_test  -> shape:\", im_test.shape,  \", dtype:\", im_test.dtype,)\n",
    "print(\"lab_train-> shape:\", lab_train.shape,  \", dtype:\", lab_train.dtype)\n",
    "print(\"lab_test -> shape:\", lab_test.shape,  \", dtype:\", lab_test.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define important parameters\n",
    "\n",
    "To avoid \"hard writing\" the **number of training and test images**, the **dimension** of the images and the **number of classes** to recognize, these parameters are retrieved from existing object attributes:\n",
    "- the `shape` attribute of the `im_train` and `im_test` tables allows to extract the number of training and test images,\n",
    "- the `size` attribute of the first training (or test) image gives the number of pixels of the images (784),<br>\n",
    "- the transformation of the `lab_test` array into a Python `set` (a set) gives the set of labels to recognize, whose size is the number of classes.\n",
    "\n",
    "complete the cell below accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_im_train = im_train.shape[0]    # number of traing images\n",
    "nb_im_test  = im_test.shape[0]     # number of test images\n",
    "nb_pixel    = im_train[0].size     # number of elements (pixels) of the firt training image\n",
    "nb_classe   = len(set(lab_test))   # number of classes to recognize (the ten digits from 0 to 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{nb_im_train} traing images and {nb_im_test} test images\")\n",
    "print(f\"{nb_pixel} pixels in each image\")\n",
    "print(f\"{nb_classe} classes to recognize (the digits from 0 to 9)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Pre-process MNIST images and labels\n",
    "\n",
    "Two treatments must be applied to the data from the MNIST database:\n",
    "- on the images: transform the matrices of  28$\\,\\times\\,$28 pixels (`uint8`integers) into **normalized** vectors $(V_i)_{i=0..783}$ of 784 real values $V_i$ with $ 0 \\leqslant V_i \\leqslant 1$;\n",
    "- on labels: transform scalar numbers into *one-hot* vectors.\n",
    "\n",
    "### Transform input matrices into normalized vectors\n",
    "\n",
    "Define the arrays `x_train` and `x_test` containing the matrices of the arrays `im_train` and `im_test` *flattened* as normalized vectors (values between 0 and 1).<br>\n",
    "*tips*:\n",
    "- use the `reshape` method of the *ndarray* class of *numpy* and the `nb_im_train`, `nb_im_test` and `nb_pixel` parameters previously defined,\n",
    "- normalization can be handled by dividing arrays by their max value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = im_train.reshape(nb_im_train, nb_pixel)/im_train.max()\n",
    "x_test  = im_test.reshape(nb_im_test, nb_pixel)/im_test.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the dimensions of the `x_train` and `x_test` arrays as well as their *min* and *max* values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train.min(), x_train.max()) == (0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_test.min(), x_test.max()) == (0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *one-hot* encoding of labels:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consult the documentation of the `to_categorical` function on the page [tf.keras.utils.to_categorical](https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical) and deduce how to define the `y_train` and `y_test` arrays containing the *hot-one* encoded version of the `lab_train` and `lab_test` arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "# 'one-hot' encoding' des labels :\n",
    "y_train = to_categorical(lab_train)\n",
    "y_test  = to_categorical(lab_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually check the first 10 values of the `lab_train` and `y_train` arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lab_train[:10])\n",
    "print(y_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Build the dense neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now build a dense **sequential** neural network in Python **5 lines** using the **keras** module.\n",
    "\n",
    "Build the network incrementally in the cell below, following the proposed approach (see the page [guide/keras/sequential_model](https://www.tensorflow.org/guide/keras/sequential_model ) if necessary) :\n",
    "- 1/ Create the object `model` instance of the class `Sequential` (cf [tf.keras.Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential)).\n",
    "- 2/ With the `add` method of the `model` object add:\n",
    "    - the input layer `Input(shape=<number of neurons>)` (cf [tf.keras.layers.Input](https://www.tensorflow.org/api_docs/python/tf/keras/Input) )<br>\n",
    "    Use the `nb_pixel` parameter to specify the value of the `shape` parameter (which must be a `tuple`)...<br>\n",
    "    - the intermediate dense layer (cf [tf.keras.layers.Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)): `Dense(<number of neurons>, activation='relu')` (cf [tf.keras.activation.relu](https://www.tensorflow.org/api_docs/python/tf/keras/activations/relu))\n",
    "    - the output dense layer: `Dense(<number of neurons>, activation='softmax')` (cf [tf.keras.activation.softmax](https://www.tensorflow.org/api_docs/python/tf/keras/activations/softmax)).<br>\n",
    "Use the `nb_pixel` and `nb_classe` parameters to indicate the number of neurons and the number of classes without 'writing them hard'...\n",
    "- 3/ Once built, the network must be compiled (in the sense of tensorflow) with the `compile` method and the arguments:\n",
    "    - `loss='categorical_crossentropy'`: choice of the error function (cf [tf.keras.categorical_crossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/categorical_crossentropy))\n",
    "    - `optimizer='adam'`: choice of Adam optimizer (see page [tf.keras.optimizers.Adam](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) )\n",
    "    - `metrics=['accuracy']` to obtain training statistics to draw performance curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "\n",
    "# set the seed of the random generators used by tensorflow:\n",
    "SEED = 1234\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# the 5 lines to build the neural network:\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(nb_pixel,), name='Input'))\n",
    "model.add(Dense(nb_pixel, activation='relu', name='C1'))\n",
    "model.add(Dense(nb_classe, activation='softmax', name='C2'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: By using the `name` argument in the `Input` and `Dense` constructors, one can give custom names to the layers, which will appear in the `summary` and `plot_model` outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `summary` method of the `model` object, display the description of the model and check the dimensions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are there some `None` in the \"Output Shape\" column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the total number of parameters with a simple formula..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "785*784+785*10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `plot_model` function draws the structure of the network (see the page [tf.keras.utils.plot_model](https://www.tensorflow.org/api_docs/python/tf/keras/utils/plot_model)). <br>\n",
    "Plot the model structure by adding the `show_shapes=True` option to the `plot_model` call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the initial DNN state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save the initial (random) values of the DNN weights with the `save_weights` method of the `Sequential` class. <br>\n",
    "This will be useful later to reset the DNN to its initial state before starting other trainings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check whether the folder 'weights' exists and create it if needed:\n",
    "if not os.path.isdir(\"weights\"): os.mkdir(\"weights\")\n",
    "\n",
    "# Save the initial DNN (random) weights:\n",
    "key = 'dense-1_init'\n",
    "model.save_weights(os.path.join('weights', key))\n",
    "\n",
    "# Display the created files:\n",
    "files=[os.path.join(\"weights\",f) for f in os.listdir(\"weights\") if f.startswith(key)]\n",
    "for f in files: print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the `save_weights` method uses the `key` part of its argument to prefix the created file names.<br>\n",
    "When loading the NDD weights later with the `load_weights` method of the `Sequential` class, just use the same key to retrieve the relevant files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarque : la méthode `save_weights` utilise la partie `key` du chemin passé en argument pour préfixer les fichiers créés.<br>\n",
    "Lors de la lecture ultérieure des poids du réseau avec la méthode `load_weights` de la classe `Sequential`, il suffira de donner la même clef pour retrouver les bons fichiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - A first network training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If necessary, consult the documentation of the `fit` method on the page [tf.keras.Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential).\n",
    "\n",
    "Complete the cell below to train the DNN with the `fit` method of the `model` object using the arguments:\n",
    "- `x_train`: the 60000 flattened and normalized images\n",
    "- `y_train`: the 60000 *one-hot* encoded labels.\n",
    "- `epochs=15`: repeat the training 15 times.\n",
    "- `batch_size=128`: split the input data set (the 60000 images) into \"batches\" of size `batch_size` (here: batches of 128 images).<br>\n",
    "Updating network weights is done after each batche of `batch_size` images.<br>\n",
    "The value of `batch_size` (by default: 32) is a parameter that influences the quality of the training but also its memory footprint: you can later try different values (64, 128, 256 ...) and observe how the quality of the training evolves).\n",
    "\n",
    "Name `hist` the data returned by the `fit` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case we execute this cell several times, we can re-initialize \n",
    "# the network to its initial state if we want to compare the workouts...\n",
    "key = 'dense-1_init'\n",
    "model.load_weights(os.path.join('weights', key)) \n",
    "\n",
    "# set the seed of the random generators inolved by tensorflow:\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# train the DNN:\n",
    "hist = model.fit(x_train, y_train, epochs=15, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you explain why there are 469 updates of the DNN weights per epoch ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" if we divide the datat set size:{len(x_train)} by the batch_szie:{128}, we get {len(x_train)/128}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `hist` object returned by the `fit` method has a `history` attribute of type `dict` whose keys `'loss'` and `'accuracy'` are associated with the corresponding values at each _epoch_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.history['accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of the `loss` and `accuracy` curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `plot_loss_accuracy` function of the `utils.tools` module (found in the notebook directory) plots the \"Model accuracy\" and \"Model loss\" curves with the data stored in `hist`.<br> Import and use the `plot_loss_accuracy` function to plot these curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tools import plot_loss_accuracy\n",
    "plot_loss_accuracy(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Train the network while measuring its performance at each *epoch*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have a better indicator of the quality of the training, you can test at each `epoch` the precision of the inferences of the trained network using the test data: just pass the `validation_data` argument to the `fit` method, assigning it the test data tuple `(x_test, y_test)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the initial state of the DNN\n",
    "key = 'dense-1_init'\n",
    "model.load_weights(os.path.join('weights', key))\n",
    "\n",
    "# set the seed of the random generators inolved by tensorflow:\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "hist = model.fit(x_train, y_train,\n",
    "                 validation_data=(x_test, y_test), \n",
    "                 epochs=15, \n",
    "                 batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the `hist.history` dictionary has also the new keys `val_loss` and `val_accuracy` calculated with the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot again the curves with the `plot_loss_accuracy` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_accuracy(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the precision calculated with the test data tends towards a limit close to 98%. You might think that increasing the value of `epochs` would improve the precision of the network... but you run the risk of over-training the network (*over-fit*)..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Train the network while measuring its performance at each *epoch* and managing the *over-fit*\n",
    "\n",
    "The `Keras` module offers tools to automatically stop the training by monitoring, for example, the growth of precision from one `epoch` to another.\n",
    "You define the parameters of the `EarlyStopping` (cf [EarlyStopping](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping)) *callback* and pass it to the method `fit` via the `callbacks` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "callbacks_list = [ \n",
    "    EarlyStopping(monitor='val_accuracy',  # the parameter to monitor\n",
    "                  mode='max',              # the parameter is supposed to grow monotonically\n",
    "                  patience=2,              # accept that the parameter decreases only twice\n",
    "                  restore_best_weights=True,\n",
    "                  verbose=1)\n",
    "]\n",
    "\n",
    "# relod the DNN initial state:\n",
    "key = 'dense-1_init'\n",
    "model.load_weights(os.path.join('weights', key))\n",
    "\n",
    "# set the seed of the random generators inolved by tensorflow:\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "hist = model.fit(x_train, y_train,\n",
    "                 validation_data=(x_test, y_test),\n",
    "                 epochs=15, \n",
    "                 batch_size=128, \n",
    "                 callbacks = callbacks_list)\n",
    "\n",
    "from utils.tools import plot_loss_accuracy\n",
    "plot_loss_accuracy(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of monitoring the decrease of `val_accuracy` you can also monitor the increase of `val_loss`, which may be a prefeerd strategy (can you guess why ?):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "callbacks_list = [ \n",
    "    EarlyStopping(monitor='val_loss',  # the parameter to monitor\n",
    "                  mode='min',          # the parameter is suspposed to decrese\n",
    "                  patience=2,          # accept that 'val_loss' increases twice\n",
    "                  restore_best_weights=True,\n",
    "                  verbose=1)\n",
    "]\n",
    "\n",
    "# relod the DNN initial state:\n",
    "key = 'dense-1_init'\n",
    "model.load_weights(os.path.join('weights', key))\n",
    "\n",
    "# set the seed of the random generators inolved by tensorflow:\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "hist = model.fit(x_train, y_train,\n",
    "                 validation_data=(x_test, y_test),\n",
    "                 epochs=15, \n",
    "                 batch_size=128, \n",
    "                 callbacks = callbacks_list)\n",
    "\n",
    "from utils.tools import plot_loss_accuracy\n",
    "plot_loss_accuracy(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 8 - Save the trained DNN\n",
    "\n",
    "The **weights** or **the structure and weights** of a trained network can be saved in a file with the `save_weights` and `save` methods of the `Sequential` class.<br>\n",
    "\n",
    "### $\\leadsto$ Save the weights of the trained DNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether the folder 'weights' exists and create it if needed:\n",
    "if not os.path.exists(\"weights\"): os.mkdir(\"weights\")\n",
    "\n",
    "# save the trained DNN weights:\n",
    "key = 'trained-1_data'\n",
    "model.save_weights(os.path.join('weights', key))\n",
    "\n",
    "# Display the created files:\n",
    "files=[os.path.join(\"weights\",f) for f in os.listdir(\"weights\") if f.startswith(key)]\n",
    "for f in files: print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\leadsto$ Save the weights AND structure of the trained DNN\n",
    "\n",
    "The `save` method of the `Sequential` class saves **the structure** and the **weights** of the trained DNN.<br>\n",
    "You can use later the `tf.keras.models.load_model` function to recreate the network and reload its trained weights to exploit it in operational situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check whether the folder 'models' exists and create it if needed:\n",
    "if not os.path.exists(\"models\"): os.mkdir(\"models\")\n",
    "\n",
    "# save the trained DNN structure + wieghts:\n",
    "key = 'trained-1_model'\n",
    "model.save(os.path.join('models', key) )\n",
    "\n",
    "# Display the created files:\n",
    "files=[os.path.join(\"models\",f) for f in os.listdir(\"models\") if f.startswith(key)]\n",
    "for f in files: print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Further work:\n",
    "You can now load the `ML2_DNN_part2_en.ipynb` notebook to learn how to exploit the DNN you have just rained.\n",
    "\n",
    "## Other interesting resources... videos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://www.youtube.com/embed/trWrEWfhTVg\" width=\"800\" height=\"450\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe src=\"https://www.youtube.com/embed/trWrEWfhTVg\" width=\"800\" height=\"450\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://www.youtube.com/embed/aircAruvnKk\" width=\"800\" height=\"450\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe src=\"https://www.youtube.com/embed/aircAruvnKk\" width=\"800\" height=\"450\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://www.youtube.com/embed/IHZwWFHWa-w\" width=\"800\" height=\"450\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe src=\"https://www.youtube.com/embed/IHZwWFHWa-w\" width=\"800\" height=\"450\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://www.youtube.com/embed/Ilg3gGewQ5U\" width=\"800\" height=\"450\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe src=\"https://www.youtube.com/embed/Ilg3gGewQ5U\" width=\"800\" height=\"450\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
