{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:10pt\">AI @ ENSPIMA_2022-2023_v1.0_Jean-Luc CHARLES (Jean-Luc.charles@ensam.eu)_CC BY-SA 4.0</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Problem-based learning\n",
    "# Training a neural network to diagnose bearing faults - part 1 / 3\n",
    "\n",
    "### Targeted learning objectives:\n",
    "Part 1:<br>\n",
    "- Know how to load files in *Matlab MAT-file* format with *Python*.\n",
    "- Know how to dimension and fill numpy ndarrays with the data of the `.mat` files\n",
    "- Know how to display a grid of data plots\n",
    "- Know how to store the numpy ndarrays in a `.npz` file\n",
    "\n",
    "Part 2:<br>\n",
    "- Know how to load a `.npz` into numpy ndarrays\n",
    "- Know how to process the temporal dataset to get a spectral dataset.\n",
    "- Know how to display a grid of spectra plots.\n",
    "\n",
    "Part 3:<br>\n",
    "- Know how to train/operate a DNN to diagnose bearing faults using a labeled temporal dataset.\n",
    "- The problem part of the APP: Know how to train/operate a DNN to diagnose bearing faults using a labeled temporal dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<span style=\"color:brown;font-family:arial;font-size:12pt\"> \n",
    "It is important to use a <span style=\"font-weight:bold;\">Python Virtual Environment</span> (PVE) for your main Python projects: <br>\n",
    "    a PVE makes it possible to control for each project the versions of the Python interpreter and the \"sensitive\" modules (like tensorflow).<br><br>\n",
    "    All the notebooks your work on must be loaded into a jupyter-notebook or a jupyter-lab launched in the PVE \n",
    "    <b><span style=\"color: rgb(100, 151, 202);\" >pyml-pm</span></b> specially created for the session.<br>\n",
    "</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 $-$ The *Case Western Reserve University* bearing dataset\n",
    "\n",
    "The bearing data used in this notebook are provided by the **Case Western Reserve University (CWRU)** on the page [engineering.case.edu/bearingdatacenter/48k-drive-end-bearing-fault-data](https://engineering.case.edu/bearingdatacenter/48k-drive-end-bearing-fault-data) . <br>\n",
    "\n",
    "The data were collected from a motor driving mechanical system under four different loads with the sampling frequency of 48 kHz:<br>\n",
    "![sdsdv](./img/CWRU-TestBench.png)<br>\n",
    "(source: https://engineering.case.edu/bearingdatacenter/apparatus-and-procedures)\n",
    "\n",
    "The bearing data set was obtained under four experimental conditions:\n",
    "- Normal condition (N)\n",
    "- with Outer race Fault (OF)\n",
    "- with Inner race Fault (IF)\n",
    "- with Roller Fault (OF).\n",
    "\n",
    "Faulted bearings were installed into the test motor and vibration data was recorded for motor loads of 0 to 3 horsepower (motor speeds of 1797 to 1720 RPM).<br>\n",
    "The faults were introduced into the drive-end bearing of the motor with fault diameters of 0.18, 0.36 and 0.54 mm, respectively.\n",
    "\n",
    "The defaults classification table is as follows:\n",
    "\n",
    "|class label|Fault type|Fault diameter|\n",
    "|:---------:|:--------:|-------------:|\n",
    "| 1         | N        | 0            |\n",
    "| 2         | RF       | 0.18         |\n",
    "| 3         | RF       | 0.36         |\n",
    "| 4         | RF       | 0.54         |\n",
    "| 5         | IF       | 0.36         |\n",
    "| 6         | IF       | 0.36         |\n",
    "| 7         | IF       | 0.54         |\n",
    "| 8         | OF       | 0.18         |\n",
    "| 9         | OF       | 0.36         |\n",
    "| 10        | OF       | 0.54         |\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 $-$ Download the the **CWRU** dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **CWRU** dataset consists in about fifty [Matlab MAT-file](https://www.mathworks.com/help/matlab/import_export/mat-file-versions.html) files that can be downloaded:\n",
    "\n",
    "- either **manually**: by clicking on the hyper-links in the page  https://engineering.case.edu/bearingdatacenter/48k-drive-end-bearing-fault-data\n",
    "- either with **Python instruction**: for example with the *wget* module, to get the `.mat` files form the directory https://engineering.case.edu/sites/default/files.\n",
    "\n",
    "By exploring the hyper-links of the page https://engineering.case.edu/bearingdatacenter/48k-drive-end-bearing-fault-data one can define the list of the .mat files involved by the previous defaults classification table:\n",
    "\n",
    "    ['98.mat', '99.mat' '100.mat', '110.mat', '111.mat', '112.mat', '123.mat', '124.mat', '125.mat', '136.mat', '137.mat', '138.mat', '175.mat', '176.mat', '177.mat', '190.mat', '191.mat', '192.mat', '202.mat', '203.mat', '204.mat', '214.mat', '215.mat', '217.mat', '227.mat', '228.mat', '229.mat', '239.mat', '240.mat', '241.mat']\n",
    "\n",
    "The following cells let you download all the required `.mat` files with some Python instructions.<br>\n",
    "\n",
    "Note : If the download of the `mat` is to slow, you can can the `mat` files already downloaded in the `pre_loaded_dataset` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the list of the wanted '.mat' files:\n",
    "CWRU_data_file = ['98.mat', '99.mat', '100.mat', \n",
    "                  '110.mat', '111.mat', '112.mat', \n",
    "                  '123.mat', '124.mat', '125.mat', \n",
    "                  '136.mat', '137.mat', '138.mat', \n",
    "                  '175.mat', '176.mat', '177.mat', \n",
    "                  '190.mat', '191.mat', '192.mat', \n",
    "                  '202.mat', '203.mat', '204.mat', \n",
    "                  '214.mat', '215.mat', '217.mat', \n",
    "                  '227.mat', '228.mat', '229.mat', \n",
    "                  '239.mat', '240.mat', '241.mat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "\n",
    "# the URL where to find the .mat files:\n",
    "url = 'https://engineering.case.edu/sites/default/files'\n",
    "\n",
    "# the directory where to store the downloaded files:\n",
    "data_dir = \"./CWRU_dataset/\"\n",
    "if not os.path.exists(data_dir): os.mkdir(data_dir)\n",
    "\n",
    "# download the files and store tem:\n",
    "for file in CWRU_data_file:\n",
    "    file_url = os.path.join(url, file)\n",
    "    target   = os.path.join(data_dir, file)\n",
    "    if not os.path.exists(target):\n",
    "        print(f\"downloading file <{file_url}> as <{target}>\")\n",
    "        wget.download(file_url, target) \n",
    "        print(\"\")\n",
    "    else:\n",
    "        print(f\"file <{target} alredy exists>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now choose your working data directory: pre_loaded_dataset or CWRU_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "#### comment one of the two lines:\n",
    "####\n",
    "\n",
    "#data_dir = \"./pre_loaded_dataset\"\n",
    "data_dir = \"./CWRU_dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the list of the `.mat` data files that are in your `data_dir` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_mat_file = [ f for f in os.listdir(data_dir) if f.endswith(\".mat\")]\n",
    "list_mat_file.sort()\n",
    "print(f\"List of .mat files in <{data_dir}>:\\n{list_mat_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 $-$ handling of the **CWRU** dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scipy.io.loadmat` can load a `.mat` file (*MAT-file* format < 7.3) and return the data as a Python `dict` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = os.path.join(data_dir, \"98.mat\")\n",
    "\n",
    "mat98 = scipy.io.loadmat(data_file)  \n",
    "mat98"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see in the above cell that the return of `loadmat` is a Python dictionary... <br><br>\n",
    " Now let's look at its **keys**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat98.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accelerometers data are associated with the keys:<br>\n",
    "- `X098_DE_time`: temporal data of the accelerometer at Drive End (DE), sampled at 48 kHz<br>\n",
    "- `X098_FE_time`: temporal data of the accelerometer at Fan End (FE), sampled at 48 kHz.<br><br>\n",
    "Accelerometer data in the dictionnary are `numpy.ndarray` objets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(mat98['X098_DE_time']), type(mat98['X098_FE_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arrays are single column matrices of accelerometers output sampled at 48 khz/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat98['X098_DE_time'].shape, mat98['X098_FE_time'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 $-$ Minimalist plot of data (`pyplot` style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's name `X_DE` and `X_FE` the accelerometers data and plot the data in 2 subplots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_DE, X_FE = mat98['X098_DE_time'], mat98['X098_FE_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(X_DE[-2000:], '.-b', markersize=0.6)\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(X_FE[-2000:], '.-m', markersize=0.6)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 $-$ More elaborate plot of data (`Axes` object style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Let's compute a time vector for abscissa:\n",
    "N = 4000                 # we take only the last 4000 temporal data points\n",
    "T = np.arange(N)/48e3\n",
    "T *= 1e3                 # cconvert T in milli-sec\n",
    "\n",
    "key_DE, key_FE = 'X098_DE_time', 'X098_FE_time'\n",
    "X_DE, X_FE = mat98[key_DE], mat98[key_FE]\n",
    "\n",
    "# min and max values for plotting:\n",
    "max_value = max(X_DE[-N:].max(), X_FE[-N:].max())\n",
    "min_value = min(X_DE[-N:].min(), X_FE[-N:].min())\n",
    "\n",
    "# subplots returns a figure and a list of Axes:\n",
    "fig, axes = plt.subplots(2,1, sharex=True) \n",
    "\n",
    "fig.suptitle(f\"Accelerometers output from file <{os.path.basename(data_file)}>\")\n",
    "fig.set_size_inches((12,5))\n",
    "\n",
    "axe = axes[0]\n",
    "axe.plot(T, X_DE[-N:], '-b', markersize=0.6, linewidth=0.5, label=key_DE)\n",
    "axe.set_ylabel(\"Arbitrary unit\")\n",
    "axe.set_ylim(min_value, max_value)\n",
    "axe.legend(loc='upper right', framealpha=0.5)\n",
    "axe.grid()\n",
    "\n",
    "axe = axes[1]\n",
    "axe.plot(T, X_FE[-N:], '-m', markersize=0.6, linewidth=0.4, label=key_FE)\n",
    "axe.set_ylabel(\"Arbitrary unit\")\n",
    "axe.set_xlabel(\"Time [ms]\")\n",
    "axe.set_ylim(min_value, max_value)\n",
    "axe.legend(loc='upper right', framealpha=0.5)\n",
    "axe.grid()\n",
    "\n",
    "plt.savefig(\"CWRU_data.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 $-$ Creating the numpy dataset from CWRU MAT-files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define 3 datasets `A`, `B` and `C` by grouping the data for motor loads 1, 2 and 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# group the CWRU files number in 3 datasets for the the motor loads 1, 2 and 3 horsepower:\n",
    "num_1 = ( 98, 123, 190, 227, 110, 175, 214, 136, 202, 239)\n",
    "num_2 = ( 99, 124, 191, 228, 111, 176, 215, 137, 203, 240)\n",
    "num_3 = (100, 125, 192, 229, 112, 177, 217, 138, 204, 241)\n",
    "\n",
    "# define 3 arrays for the 3 datasets above: \n",
    "# for each of the 10 health condidion we will split the full dataset in 200 samples of 1900 points,\n",
    "# so the shape of each array is(10, 200, 1900):\n",
    "nb_HC       = 10\n",
    "nb_sample   = 200\n",
    "sample_size = 1900\n",
    "A = np.zeros((nb_HC, nb_sample, sample_size), dtype=float)\n",
    "B = np.zeros((nb_HC, nb_sample, sample_size), dtype=float)\n",
    "C = np.zeros((nb_HC, nb_sample, sample_size), dtype=float)\n",
    "\n",
    "# loop simultaneously accross the files numbers and the dataset arrays to fill the arrays\n",
    "# with the files data:\n",
    "for numbers, array in zip((num_1, num_2, num_3), (A, B, C)):\n",
    "    \n",
    "    for hc, file_num in enumerate(numbers):\n",
    "        # hc is the health condition rank in [0,9]\n",
    "        \n",
    "        # build the 'mat' file path:\n",
    "        file = os.path.join(data_dir,f\"{file_num}.mat\")\n",
    "        print(f\"Loading file <{file:8s}>...\", end=\"\")\n",
    "        \n",
    "        # load the data of the file in the dict 'data':\n",
    "        data = scipy.io.loadmat(file) \n",
    "        \n",
    "        # build the key and get the data we want from the dictionnary:\n",
    "        key = f\"X{file_num:03d}_DE_time\"\n",
    "        X = data[key]\n",
    "        print(f' got values for key {key}, shape:{X.shape}')\n",
    "        \n",
    "        # Try to split the data acroos the array dimensions:\n",
    "        try:\n",
    "            for s in range(nb_sample):\n",
    "                # s is the sample number\n",
    "                array[hc, s] = X[s*sample_size:(s+1)*sample_size, 0]\n",
    "        except:\n",
    "            print(f\"Error wit file <{file_num}.mat>\")\n",
    "    print('-'*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 $-$ Plot the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot all the data for sample #0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the list of the health condition labels:\n",
    "health_cond = ['N']\n",
    "for def_type in 'RF', 'IF', 'OF':\n",
    "    for size in '18', '36', '54':\n",
    "        health_cond.append(f\"{def_type}.{size}\")\n",
    "print(f\"list of {len(health_cond)} health conditions:\", health_cond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define 'nb_HC', the number of health conditions:\n",
    "nb_HC = len(health_cond)\n",
    "\n",
    "# define 'nb_L', the number of load cases:\n",
    "full_dataset = (A, B, C)\n",
    "nb_L = len(full_dataset)\n",
    "\n",
    "s = 0  # the sample number\n",
    "\n",
    "plt.rcParams['font.size'] = 6   # change the pyplot defaut font size\n",
    "fig, axes = plt.subplots(nb_HC, nb_L, sharex=True)\n",
    "fig.set_size_inches((8,12))\n",
    "plt.subplots_adjust(top=.95, wspace=0.25, hspace=0.5)\n",
    "plt.suptitle(f\"Plots for the sample #{s}\", fontsize=10)\n",
    "for n, dataset in enumerate(full_dataset):\n",
    "    for h in range(nb_HC):\n",
    "        axe = axes[h,n]\n",
    "        axe.set_title(f\"Load_{n+1} / health cond {health_cond[h]}\", fontsize=8)\n",
    "        axe.plot(dataset[h, s], linewidth=0.4)\n",
    "        if h == nb_HC-1: axe.set_xlabel(\"Rank\")\n",
    "plt.rcParams['font.size'] = 10  # restore the pyplot defaut font size to its defautl value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 $-$ Export the numpy dadasets in a `.npz` compressed file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('CWRU_dadaset', A, B, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
